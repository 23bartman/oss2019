---
title        : Dealing with Security Findings in the Enterprise
type         : working-session
track        : Security Automation
topics       : ["SDL"]
technology   : Dependency Check, FindSecBugs, ZAP, Defect Dojo
categories   :                      # GDPR, Juice Shop, etc.
featured     : yes                  # review with summit team "yes"
when_day     : Tue
when_time    : PM-1, PM-2
room_layout  :                    #
room_id      : room-5
session_slack: 
status       : review-content              # draft, review-content, done
organizers   : Claudio Camerino
participants : 
description  : How to deal with the security findings generated by security tools as part of CI/CD pipeline
---


## Why

Thanks to the proliferation of autonated security scanning tools we are generating a phenomenal amount of security findings. As part of the this session we tackle the following goals

1. Visibility - Can't secure what you don't see. Why is important to test early in the SDL and map tests to business flows
2. Accountability - Creating a feedback loop. Why is important to flag findings to their respective owners
3. Noise Removal - Accuracy drives credibility. Developers are more likely to triage and action reputable findings, starting with tigther scan policies
4. Scalability - Manual configuration of tools, polocies and processes is not an option when dealing with hundreds of products. How to scale generation, collection and triaging of security findings.

## What

- What tools and test types should be used to generate security findings? What's a good toolset to start from?
- The appsec workflow: generate, collect, consolidate, sanitise, allocate, triage, accept/reject/fix and track security findings.
- Different approaches in security tests for frontend and backend applications, what should scan policies test for and how?
- Integration with Jira - how to raise and populate a SEC type ticket. What information should go in there and how to track its lifecycle.
- AppSec testing integration with QA - User stories vs abuse cases and how to leverage QA processes to drive DAST tools
- Continous improvement - is it possible to tune scan policies as part of the triage process?

## Outcomes

- Build a POC for the system
- Generates/amend scripts to automate generation, collectin and allocation of findings
- Define ruleset for prgrammatic removal of noise (i.e. duplicates, transitive dependendencies and easy to sport FPs)
- Adapt/hack OSS tools like ZAP, findsecbugs, dependency check and Defect Dojo for enterprise level automation
- Define roles and responsibilities for these activittes based on common industry roles (QA, Del Svcs, Engineering etc.)
- Generation of DAST scan policies and SAST quality profiles (for Sonarqube) for common technologies and frameworks
- E2E POC (TBC)

## Who

The target audience for this Working Session is:

 - Developers
 - Security professionals
 - DevSecOps
 - Security champions
 - AppSec leaders

## Working materials

Here are the current 'work in progress' materials for this session (please add as much information as possible before the sessions):

- [The Security Development Lifecycle](https://www.owasp.org/images/7/78/OWASP_AppSec_Research_2010_Keynote_2_by_Lipner.pdf)
- [SDL in Practice](https://www.owasp.org/images/4/45/SDL_in_practice.pdf)
- [Defect Dojo](https://github.com/DefectDojo/django-DefectDojo)
- [OWASP ZAP](https://github.com/zaproxy/zaproxy)
- [Dependency Check](https://github.com/jeremylong/DependencyCheck)

## Previous Summit Working Session
